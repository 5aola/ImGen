{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd93fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries to load and process the locally downloaded COCO dataset\n",
    "import json, os, random, math\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from skimage.transform import resize as imresize\n",
    "import pycocotools.mask as mask_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7470ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "INV_IMAGENET_MEAN = [-m for m in IMAGENET_MEAN]\n",
    "INV_IMAGENET_STD = [1.0 / s for s in IMAGENET_STD]\n",
    "\n",
    "def imagenet_preprocess():\n",
    "  return T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "# To resize the pictures to the same resolution (in our case we set the resolution 64x64)\n",
    "class Resize(object):\n",
    "  def __init__(self, size, interp=PIL.Image.BILINEAR): \n",
    "    if isinstance(size, tuple):\n",
    "      H, W = size\n",
    "      self.size = (W, H)\n",
    "    else:\n",
    "      self.size = (size, size)\n",
    "    self.interp = interp\n",
    "\n",
    "  def __call__(self, img):\n",
    "    return img.resize(self.size, self.interp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abf39af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CocoSceneGraphDataset class gets and prepares the downloaded dataset for training\n",
    "\n",
    "class CocoSceneGraphDataset(Dataset):\n",
    "  def __init__(self, image_dir, instances_json, \n",
    "               image_size=(64, 64), mask_size=16,\n",
    "               normalize_images=True,\n",
    "               min_object_size=0.02,\n",
    "               min_objects_per_image=1, max_objects_per_image=4,):\n",
    "    \"\"\"\n",
    "    Loading images and annotations then converting them to scene graphs on the fly.\n",
    "\n",
    "    Inputs:\n",
    "    - image_dir: Path to a directory where images are held\n",
    "    - instances_json: Path to a JSON file giving COCO annotations\n",
    "    - image_size: Size (H, W) at which to load images. Default (64, 64).\n",
    "    - mask_size: Size M for object segmentation masks; default 16.\n",
    "    - normalize_image: If True then normalize images by subtracting ImageNet\n",
    "      mean pixel and dividing by ImageNet std pixel.\n",
    "    - include_relationships: If True then include spatial relationships; if\n",
    "      False then only include the trivial __in_image__ relationship.\n",
    "    - min_object_size: Ignore objects whose bounding box takes up less than\n",
    "      this fraction of the image.\n",
    "    - min_objects_per_image: Ignore images which have fewer than this many\n",
    "      object annotations.\n",
    "    - max_objects_per_image: Ignore images which have more than this many\n",
    "      object annotations.\n",
    "\n",
    "    \"\"\"\n",
    "    super(Dataset, self).__init__()\n",
    "\n",
    "    self.image_dir = image_dir\n",
    "    self.mask_size = mask_size\n",
    "    self.max_samples = None\n",
    "    self.normalize_images = normalize_images\n",
    "    self.set_image_size(image_size)\n",
    "\n",
    "    with open(instances_json, 'r') as f:\n",
    "      instances_data = json.load(f)\n",
    "\n",
    "    self.image_ids = []\n",
    "    self.image_id_to_filename = {}\n",
    "    self.image_id_to_size = {}\n",
    "    for image_data in instances_data['images']:\n",
    "      image_id = image_data['id']\n",
    "      filename = image_data['file_name']\n",
    "      width = image_data['width']\n",
    "      height = image_data['height']\n",
    "      self.image_ids.append(image_id)\n",
    "      self.image_id_to_filename[image_id] = filename\n",
    "      self.image_id_to_size[image_id] = (width, height)\n",
    "    \n",
    "    # in vocab we store the objects and relationships name  \n",
    "    # and the belonging IDs\n",
    "    self.vocab = {\n",
    "      'object_name_to_idx': {},\n",
    "      'pred_name_to_idx': {},\n",
    "    }\n",
    "\n",
    "    object_idx_to_name = {}\n",
    "    all_instance_categories = []\n",
    "    for category_data in instances_data['categories']:\n",
    "      category_id = category_data['id']\n",
    "      category_name = category_data['name']\n",
    "      all_instance_categories.append(category_name)\n",
    "      object_idx_to_name[category_id] = category_name\n",
    "      self.vocab['object_name_to_idx'][category_name] = category_id\n",
    "    all_stuff_categories = []\n",
    "    \n",
    "    instance_whitelist = all_instance_categories\n",
    "    stuff_whitelist = all_stuff_categories\n",
    "    category_whitelist = set(instance_whitelist) | set(stuff_whitelist)\n",
    "\n",
    "    # Add object data from instances\n",
    "    self.image_id_to_objects = defaultdict(list)\n",
    "    for object_data in instances_data['annotations']:\n",
    "      image_id = object_data['image_id']\n",
    "      _, _, w, h = object_data['bbox']\n",
    "      W, H = self.image_id_to_size[image_id]\n",
    "      box_area = (w * h) / (W * H)\n",
    "      box_ok = box_area > min_object_size\n",
    "      object_name = object_idx_to_name[object_data['category_id']]\n",
    "      category_ok = object_name in category_whitelist\n",
    "      other_ok = object_name != 'other'\n",
    "      if box_ok and category_ok and other_ok:\n",
    "        self.image_id_to_objects[image_id].append(object_data)\n",
    "\n",
    "    # COCO category labels start at 1, so use 0 for __image__\n",
    "    self.vocab['object_name_to_idx']['__image__'] = 0\n",
    "\n",
    "    # Build object_idx_to_name\n",
    "    name_to_idx = self.vocab['object_name_to_idx']\n",
    "    assert len(name_to_idx) == len(set(name_to_idx.values()))\n",
    "    max_object_idx = max(name_to_idx.values())\n",
    "    idx_to_name = ['NONE'] * (1 + max_object_idx)\n",
    "    for name, idx in self.vocab['object_name_to_idx'].items():\n",
    "      idx_to_name[idx] = name\n",
    "    self.vocab['object_idx_to_name'] = idx_to_name\n",
    "\n",
    "    # Prune images that have too few or too many objects\n",
    "    new_image_ids = []\n",
    "    total_objs = 0\n",
    "    for image_id in self.image_ids:\n",
    "      num_objs = len(self.image_id_to_objects[image_id])\n",
    "      total_objs += num_objs\n",
    "      if min_objects_per_image <= num_objs <= max_objects_per_image:\n",
    "        new_image_ids.append(image_id)\n",
    "    self.image_ids = new_image_ids\n",
    "    \n",
    "    self.vocab['pred_idx_to_name'] = [\n",
    "      '__in_image__',\n",
    "      'left of',\n",
    "      'right of',\n",
    "      'above',\n",
    "      'below',\n",
    "      'inside',\n",
    "      'surrounding',\n",
    "    ]\n",
    "    self.vocab['pred_name_to_idx'] = {}\n",
    "    for idx, name in enumerate(self.vocab['pred_idx_to_name']):\n",
    "      self.vocab['pred_name_to_idx'][name] = idx\n",
    "\n",
    "  def set_image_size(self, image_size):\n",
    "    print('called set_image_size', image_size)\n",
    "    transform = [Resize(image_size), T.ToTensor()]\n",
    "    if self.normalize_images:\n",
    "      transform.append(imagenet_preprocess())\n",
    "    self.transform = T.Compose(transform)\n",
    "    self.image_size = image_size\n",
    "\n",
    "  def total_objects(self):\n",
    "    total_objs = 0\n",
    "    for i, image_id in enumerate(self.image_ids):\n",
    "      if self.max_samples and i >= self.max_samples:\n",
    "        break\n",
    "      num_objs = len(self.image_id_to_objects[image_id])\n",
    "      total_objs += num_objs\n",
    "    return total_objs\n",
    "\n",
    "  def __len__(self):\n",
    "    if self.max_samples is None:\n",
    "      return len(self.image_ids)\n",
    "    return min(len(self.image_ids), self.max_samples)\n",
    "\n",
    "  def getimageID(self, index):\n",
    "        return self.image_ids[index]\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Get the pixels of an image, and a random synthetic scene graph for that\n",
    "    image constructed on-the-fly from its COCO object annotations. We assume\n",
    "    that the image will have height H, width W, C channels; there will be O\n",
    "    object annotations, each of which will have both a bounding box and a\n",
    "    segmentation mask of shape (M, M). There will be T triples in the scene\n",
    "    graph.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - image: FloatTensor of shape (C, H, W)\n",
    "    - objs: LongTensor of shape (O,)\n",
    "    - boxes: FloatTensor of shape (O, 4) giving boxes for objects in\n",
    "      (x0, y0, x1, y1) format, in a [0, 1] coordinate system\n",
    "    - masks: LongTensor of shape (O, M, M) giving segmentation masks for\n",
    "      objects, where 0 is background and 1 is object.\n",
    "    - triples: LongTensor of shape (T, 3) where triples[t] = [i, p, j]\n",
    "      means that (objs[i], p, objs[j]) is a triple.\n",
    "    \"\"\"\n",
    "    image_id = self.image_ids[index]\n",
    "    \n",
    "    #print(\"image id: \" + str(image_id))\n",
    "    \n",
    "    filename = self.image_id_to_filename[image_id]\n",
    "    image_path = os.path.join(self.image_dir, filename)\n",
    "    with open(image_path, 'rb') as f:\n",
    "      with PIL.Image.open(f) as image:\n",
    "        WW, HH = image.size\n",
    "        image = self.transform(image.convert('RGB'))\n",
    "\n",
    "    H, W = self.image_size\n",
    "    objs, boxes, masks = [], [], []\n",
    "    for object_data in self.image_id_to_objects[image_id]:\n",
    "      objs.append(object_data['category_id'])\n",
    "      x, y, w, h = object_data['bbox']\n",
    "      x0 = x / WW\n",
    "      y0 = y / HH\n",
    "      x1 = (x + w) / WW\n",
    "      y1 = (y + h) / HH\n",
    "      boxes.append(torch.FloatTensor([x0, y0, x1, y1]))\n",
    "\n",
    "      # This will give a numpy array of shape (HH, WW)\n",
    "      mask = seg_to_mask(object_data['segmentation'], WW, HH)\n",
    "\n",
    "      # Crop the mask according to the bounding box, being careful to\n",
    "      # ensure that we don't crop a zero-area region\n",
    "      mx0, mx1 = int(round(x)), int(round(x + w))\n",
    "      my0, my1 = int(round(y)), int(round(y + h))\n",
    "      mx1 = max(mx0 + 1, mx1)\n",
    "      my1 = max(my0 + 1, my1)\n",
    "      mask = mask[my0:my1, mx0:mx1]\n",
    "      mask = imresize(255.0 * mask, (self.mask_size, self.mask_size),\n",
    "                      mode='constant')\n",
    "      mask = torch.from_numpy((mask > 128).astype(np.int64))\n",
    "      masks.append(mask)\n",
    "\n",
    "    # Add dummy __image__ object\n",
    "    objs.append(self.vocab['object_name_to_idx']['__image__'])\n",
    "    boxes.append(torch.FloatTensor([0, 0, 1, 1]))\n",
    "    masks.append(torch.ones(self.mask_size, self.mask_size).long())\n",
    "\n",
    "    objs = torch.LongTensor(objs)\n",
    "    boxes = torch.stack(boxes, dim=0)\n",
    "    masks = torch.stack(masks, dim=0)\n",
    "\n",
    "    box_areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "    # Compute centers of all objects\n",
    "    obj_centers = []\n",
    "    _, MH, MW = masks.size()\n",
    "    for i, obj_idx in enumerate(objs):\n",
    "      x0, y0, x1, y1 = boxes[i]\n",
    "      mask = (masks[i] == 1)\n",
    "      xs = torch.linspace(x0, x1, MW).view(1, MW).expand(MH, MW)\n",
    "      ys = torch.linspace(y0, y1, MH).view(MH, 1).expand(MH, MW)\n",
    "      if mask.sum() == 0:\n",
    "        mean_x = 0.5 * (x0 + x1)\n",
    "        mean_y = 0.5 * (y0 + y1)\n",
    "      else:\n",
    "        mean_x = xs[mask].mean()\n",
    "        mean_y = ys[mask].mean()\n",
    "      obj_centers.append([mean_x, mean_y])\n",
    "    obj_centers = torch.FloatTensor(obj_centers)\n",
    "\n",
    "    # Add triples\n",
    "    triples = []\n",
    "    num_objs = objs.size(0)\n",
    "    __image__ = self.vocab['object_name_to_idx']['__image__']\n",
    "    real_objs = []\n",
    "    if num_objs > 1:\n",
    "      real_objs = (objs != __image__).nonzero().squeeze(1)\n",
    "    for cur in real_objs:\n",
    "      choices = [obj for obj in real_objs if obj != cur]\n",
    "      if len(choices) == 0:\n",
    "        break\n",
    "      other = random.choice(choices)\n",
    "      if random.random() > 0.5:\n",
    "        s, o = cur, other\n",
    "      else:\n",
    "        s, o = other, cur\n",
    "\n",
    "      # Check for inside / surrounding\n",
    "      sx0, sy0, sx1, sy1 = boxes[s]\n",
    "      ox0, oy0, ox1, oy1 = boxes[o]\n",
    "      d = obj_centers[s] - obj_centers[o]\n",
    "      theta = math.atan2(d[1], d[0])\n",
    "\n",
    "      if sx0 < ox0 and sx1 > ox1 and sy0 < oy0 and sy1 > oy1:\n",
    "        p = 'surrounding'\n",
    "      elif sx0 > ox0 and sx1 < ox1 and sy0 > oy0 and sy1 < oy1:\n",
    "        p = 'inside'\n",
    "      elif theta >= 3 * math.pi / 4 or theta <= -3 * math.pi / 4:\n",
    "        p = 'left of'\n",
    "      elif -3 * math.pi / 4 <= theta < -math.pi / 4:\n",
    "        p = 'above'\n",
    "      elif -math.pi / 4 <= theta < math.pi / 4:\n",
    "        p = 'right of'\n",
    "      elif math.pi / 4 <= theta < 3 * math.pi / 4:\n",
    "        p = 'below'\n",
    "      p = self.vocab['pred_name_to_idx'][p]\n",
    "      triples.append([s, p, o])\n",
    "\n",
    "    # Add __in_image__ triples\n",
    "    O = objs.size(0)\n",
    "    in_image = self.vocab['pred_name_to_idx']['__in_image__']\n",
    "    for i in range(O - 1):\n",
    "      triples.append([i, in_image, O - 1])\n",
    "    \n",
    "    triples = torch.LongTensor(triples)\n",
    "    return image, objs, boxes, masks, triples\n",
    "    \n",
    "# For decoding segmentation masks using the pycocotools API:\n",
    "def seg_to_mask(seg, width=1.0, height=1.0): \n",
    "  if type(seg) == list:\n",
    "    rles = mask_utils.frPyObjects(seg, height, width)\n",
    "    rle = mask_utils.merge(rles)\n",
    "  elif type(seg['counts']) == list:\n",
    "    rle = mask_utils.frPyObjects(seg, height, width)\n",
    "  else:\n",
    "    rle = seg\n",
    "  return mask_utils.decode(rle)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddebda3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called set_image_size (64, 64)\n",
      "Training dataset has 86375 images and 185516 objects\n",
      "(2.15 objects per image)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Because COCO is a much bigger dataset than we need so we only downloaded the training dataset \n",
    "We only exported 10000 images from it that we are going to use for training, validating and testing.\n",
    "\n",
    "links for the dataset:\n",
    "    The images:       http://images.cocodataset.org/zips/train2017.zip\n",
    "    The annotations:  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "\"\"\"\n",
    "dset_kwargs = {\n",
    "    'image_dir': \"../train2017/train2017\",\n",
    "    'instances_json': \"../annotations_trainval2017/annotations/instances_train2017.json\"  \n",
    "  }\n",
    "   \n",
    "train_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
    "num_objs = train_dset.total_objects()\n",
    "num_imgs = len(train_dset)\n",
    "print('Training dataset has %d images and %d objects' % (num_imgs, num_objs))\n",
    "print('(%.2f objects per image)' % (float(num_objs) / num_imgs))\n",
    "\n",
    "#val_dset = CocoSceneGraphDataset(**dset_kwargs)\n",
    "#assert train_dset.vocab == val_dset.vocab\n",
    "#vocab = json.loads(json.dumps(train_dset.vocab))\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26416ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4739, -0.3712,  1.2899,  ..., -1.0219, -1.3987, -1.3644],\n",
       "          [-0.4739, -0.4226,  0.9817,  ..., -0.9534, -1.3130, -1.2788],\n",
       "          [-0.4739, -0.4397,  0.6049,  ..., -1.0733, -1.2959, -1.2445],\n",
       "          ...,\n",
       "          [-0.8849, -0.8507, -0.8164,  ..., -1.6898, -1.9295, -2.0665],\n",
       "          [-0.9020, -0.8678, -0.8335,  ..., -1.8610, -2.0665, -2.1008],\n",
       "          [-0.9192, -0.8849, -0.8507,  ..., -2.0494, -2.1008, -2.1008]],\n",
       " \n",
       "         [[-0.3725, -0.2850,  1.4832,  ..., -1.2129, -1.3529, -1.2304],\n",
       "          [-0.3725, -0.3375,  1.1506,  ..., -0.9503, -1.1779, -1.1429],\n",
       "          [-0.3725, -0.3375,  0.7479,  ..., -0.7227, -1.0553, -1.1078],\n",
       "          ...,\n",
       "          [-0.7927, -0.7577, -0.7227,  ..., -1.7031, -1.8606, -2.0007],\n",
       "          [-0.8102, -0.7752, -0.7402,  ..., -1.8081, -2.0007, -2.0182],\n",
       "          [-0.8277, -0.7927, -0.7577,  ..., -1.9832, -2.0182, -2.0182]],\n",
       " \n",
       "         [[-0.1835, -0.0615,  1.8208,  ..., -1.0550, -1.1247, -0.9678],\n",
       "          [-0.2184, -0.1312,  1.4897,  ..., -0.6367, -0.8807, -0.8458],\n",
       "          [-0.2184, -0.1487,  1.0714,  ..., -0.1487, -0.5844, -0.8110],\n",
       "          ...,\n",
       "          [-0.6018, -0.5844, -0.5670,  ..., -1.6302, -1.7173, -1.7696],\n",
       "          [-0.6193, -0.6018, -0.5844,  ..., -1.6824, -1.7696, -1.7870],\n",
       "          [-0.6367, -0.6018, -0.6018,  ..., -1.7696, -1.7870, -1.7870]]]),\n",
       " tensor([70, 81,  0]),\n",
       " tensor([[0.1045, 0.3962, 0.3508, 0.9376],\n",
       "         [0.5113, 0.3656, 1.0000, 0.8645],\n",
       "         [0.0000, 0.0000, 1.0000, 1.0000]]),\n",
       " tensor([[[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       " \n",
       "         [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]),\n",
       " tensor([[0, 1, 1],\n",
       "         [1, 2, 0],\n",
       "         [0, 0, 2],\n",
       "         [1, 0, 2]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only for testing and inspetcing what does it look like exactly a processed item\n",
    "train_dset.__getitem__(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9546876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GenerateSceneGraph function exports the processed data to jsons\n",
    "# we use the images ID's to connect later the exported objects\n",
    "def GenerateSceneGraph(num):\n",
    "    \n",
    "    for n in range(num):\n",
    "        tensors = train_dset.__getitem__(n)\n",
    "        relations = tensors[-1]\n",
    "        objectids = tensors[1]\n",
    "        \n",
    "        image = tensors[0]\n",
    "        maskCoords = tensors[2]\n",
    "        masks = tensors[3]\n",
    "        \n",
    "        # First we get and collect the triples (relasionships between objects)\n",
    "        relationArray = []\n",
    "        for i in range(len(relations)):\n",
    "            relation = relations[i]\n",
    "            obj1 = vocab['object_idx_to_name'][objectids[relation[0]]]\n",
    "            rel = vocab['pred_idx_to_name'][relation[1]]\n",
    "            obj2 = vocab['object_idx_to_name'][objectids[relation[2]]]\n",
    "            relarray = [obj1, rel ,obj2]\n",
    "            relationArray.append(relarray) \n",
    "            \n",
    "        # and then exporting the relations TO json\n",
    "        x = { \"ID\" : train_dset.getimageID(n),\n",
    "            \"relationships\": relationArray}\n",
    "        with open('sceneGraphs.json', 'a') as f:\n",
    "            json.dump(x, f)\n",
    "        \n",
    "        # Also we have to export the cropped and resized images\n",
    "        imageJson = { \"ID\" : train_dset.getimageID(n),\n",
    "            \"image\": image.tolist()}\n",
    "\n",
    "        with open('images.json', 'a') as f:\n",
    "            json.dump(imageJson, f)\n",
    "            \n",
    "        # we are exporting the coordinates of rectangles that \n",
    "        # contains the objects to know where they are on the images\n",
    "        # and also the masks that shows where's the object exactly\n",
    "        maskJson = { \"ID\" : train_dset.getimageID(n),\n",
    "            \"maskcords\": maskCoords.tolist(),\n",
    "            \"masks\" :  masks.tolist()}\n",
    "        \n",
    "        with open('masks.json', 'a') as f:\n",
    "            json.dump(maskJson, f)\n",
    "            \n",
    "        # And finally we collect the images ID's too\n",
    "        with open('imagelist.json', 'a') as f:\n",
    "            json.dump({\"ID\" : train_dset.getimageID(n)}, f)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f45f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenerateSceneGraph(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36425438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
